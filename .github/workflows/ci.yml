name: CI

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always

jobs:
  # Basic checks that don't require GPU (runs on free runners)
  check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

      - name: Check with Clippy (pedantic)
        run: cargo clippy -- -W clippy::pedantic

      - name: Check formatting
        run: cargo fmt --all -- --check

      - name: Run tests (no CUDA)
        run: cargo test --all

  # GPU tests using GitHub's hosted GPU runners
  # Requires GitHub Team or Enterprise Cloud plan
  # See: https://github.blog/changelog/2024-07-08-github-actions-gpu-hosted-runners-are-now-generally-available/
  gpu-test:
    runs-on: ubuntu-gpu-t4  # NVIDIA T4, 16GB VRAM, 4 vCPU, 28GB RAM
    needs: check  # Only run after basic checks pass
    steps:
      - uses: actions/checkout@v4

      - name: Check CUDA availability
        run: nvidia-smi

      - name: Cache CUDA toolkit
        id: cache-cuda
        uses: actions/cache@v4
        with:
          path: ~/cuda-toolkit
          key: cuda-toolkit-12-6-ubuntu2404-v3

      - name: Install CUDA toolkit
        if: steps.cache-cuda.outputs.cache-hit != 'true'
        run: |
          sudo apt-get update
          # Install CUDA toolkit AND cuBLAS (separate package)
          sudo apt-get install -y cuda-toolkit-12-6 libcublas-12-6 libcublas-dev-12-6
          mkdir -p ~/cuda-toolkit/cuda ~/cuda-toolkit/cublas
          cp -a /usr/local/cuda-12.6/* ~/cuda-toolkit/cuda/
          # cuBLAS libraries are installed to system lib path
          cp -a /usr/lib/x86_64-linux-gnu/libcublas* ~/cuda-toolkit/cublas/ 2>/dev/null || true
          cp -a /usr/lib/x86_64-linux-gnu/libcublasLt* ~/cuda-toolkit/cublas/ 2>/dev/null || true

      - name: Restore CUDA toolkit from cache
        if: steps.cache-cuda.outputs.cache-hit == 'true'
        run: |
          sudo mkdir -p /usr/local/cuda-12.6
          sudo cp -a ~/cuda-toolkit/cuda/* /usr/local/cuda-12.6/
          # Restore cuBLAS to system lib path
          sudo cp -a ~/cuda-toolkit/cublas/* /usr/lib/x86_64-linux-gnu/ 2>/dev/null || true
          sudo ldconfig

      - name: Set CUDA environment
        run: |
          echo "/usr/local/cuda-12.6/bin" >> $GITHUB_PATH
          echo "LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH" >> $GITHUB_ENV
          sudo ldconfig /usr/local/cuda-12.6/lib64

      - name: Install rustup (toolchain from rust-toolchain.toml)
        run: |
          curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --default-toolchain none
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-gpu-cargo-${{ hashFiles('**/Cargo.lock') }}

      - name: Build with CUDA
        run: cargo build --features cuda

      - name: Run CUDA tests
        run: cargo test --features cuda

      # Optional: smoke test with a real model
      # Uncomment and set LLAMA_MODEL_PATH secret to enable
      # - name: Run generation example
      #   env:
      #     LLAMA_MODEL_PATH: ${{ secrets.LLAMA_MODEL_PATH }}
      #   if: ${{ env.LLAMA_MODEL_PATH != '' }}
      #   run: |
      #     cargo run --example generate --features cuda -- \
      #       --model "$LLAMA_MODEL_PATH" \
      #       --max-tokens 10 \
      #       "Hello"